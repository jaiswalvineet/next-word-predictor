---
title: "Next Word Predictor-Summary"
author: "Vineet Jaiswal"
date: "24 December 2017"
output: html_document
---

## Next word predictor 

### Objective 

The goal of this exercise is to create a product to highlight the prediction algorithm that you have built and to provide an interface that can be accessed by others. For this project you must submit:

1. A Shiny app that takes as input a phrase (multiple words) in a text box input and outputs a prediction of the next word.
2. A slide deck consisting of no more than 5 slides created with R Studio Presenter (https://support.rstudio.com/hc/en-us/articles/200486468-Authoring-R-Presentations) pitching your algorithm and app as if you were presenting to your boss or an investor.

#### Points to consider

1. How can you efficiently store an n-gram model (think Markov Chains)?
2. How can you use the knowledge about word frequencies to make your model smaller and more efficient?
3. How many parameters do you need (i.e. how big is n in your n-gram model)?
4. Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data) ?
5. How do you evaluate whether your model is any good?
6. How can you use backoff models to estimate the probability of unobserved n-grams?
7. How does the model perform for different choices of the parameters and size of the model?
8. How much does the model slow down for the performance you gain?
9. Does perplexity correlate with the other measures of accuracy?
10. Can you reduce the size of the model (number of parameters) without reducing performance?
11. What are some alternative data sets you could consider using?
12. What are ways in which the n-gram model may be inefficient?
13. What are the most commonly missed n-grams? Can you think of a reason why they would be missed and fix that?
14. What are some other things that other people have tried to improve their model?
15. Can you estimate how uncertain you are about the words you are predicting?
16. What are the most interesting ways you could show off your algorithm?
17. Are there any data visualizations you think might be helpful (look at the Swiftkey data dashboard if you have it loaded on your phone)?
18. How should you document the use of your data product (separately from how you created it) so that others can rapidly deploy your algorithm?

## How I achived it 

### Exploratory Analysis and Sampling 
* Loaded the data and found the data is very big and it will not be efficient if we take all records
* Mix all three files and take sample, took only 2 % of data for better performance of shinny application
* Before creating ngram model, we clean all the data like remove digit, puntuation, non-ASCII characters, bad words and other not required texts

### N-Gram creation and storage 
* Using tidytext methodology[5] which gives better result than tm package for sentimental analysis etc, created ngram models, in this case we created 5-gram model
* Done analysis and check if all the n-gram model created correctly 
* Save the model in csv files so it will reduce a lot of time to read on shiny server 

### Smoothing and Backoff methods
* Initially I tried Kartz but it takes too much time on shiny server 
* Load all the csv files and implement stupid backoff[6] model which in less expensive than Katz backoff and gives better accuracy, In Stupid Backoff, we use the 5-gram if we have enough data points to make it seem credible, otherwise if we don't have enough of a 5-gram count, we back-off and use the 4-gram, and if there still isn't enough of a 4-gram count then go to lower till we reached to unigram

### Shiny Web application to show
* Most predicted word with probability 
* Word Cloud of predicted word
* Predicted word in table format

### Useful links 
1. Katzs Backoff Model Implementation, https://thachtranerc.wordpress.com/2016/04/12/katzs-backoff-model-implementation-in-r/
2. Discounting methods: Part 1 - Katz Bigram, https://www.youtube.com/watch?v=hsHw9F3UuAQ
3. N-Gram, Speech and Language Processing, https://web.stanford.edu/~jurafsky/slp3/
4. NLP Notes, https://gist.github.com/ttezel/4138642 
5. N-Gram creation, https://www.tidytextmining.com/
6. Stupid backoff explained, http://www.aclweb.org/anthology/D07-1090.pdf

